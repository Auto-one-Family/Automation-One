# Bugs Found

> **Letzte Aktualisierung:** 2025-12-26
> **Gepr√ºft von:** KI-Agent (Claude)

---

## Zusammenfassung

| Kategorie | Anzahl | Priorit√§t |
|-----------|--------|-----------|
| ~~**Runtime Bugs (Server Crashes)**~~ | ~~2~~ | üü¢ Fixed |
| ~~Fehlgeschlagene Tests~~ | ~~1~~ | üü¢ Fixed |
| ~~**Log-Spam Bugs**~~ | ~~1~~ | üü¢ Fixed |
| **Zombie-Prozesse/Graceful Shutdown** | 1 | üî¥ Critical |
| **MQTT Connection Leak** | 1 | üü° Medium |
| **MQTT Verbindungs-Bug** | 1 | üü° Medium |
| Deprecation Warnings | 3 | üü° Medium |
| Konfiguration/Setup | 2 | üîµ Low (Dev Only) |
| Code Coverage | 1 | üîµ Low |

---

## üî¥ CRITICAL: Zombie-Prozesse und fehlendes Graceful Shutdown

### Bug E: Mehrere Server-Instanzen laufen parallel / Graceful Shutdown fehlt

**Status:** ‚ö†Ô∏è OPEN
**Datei:** `El Servador/god_kaiser_server/src/main.py` (Lifespan)
**Priorit√§t:** üî¥ Critical - Ressourcen-Leak, Port-Konflikte

#### Beschreibung
Beim Beenden des Servers (Ctrl+C oder Prozess-Kill) werden nicht alle Child-Prozesse und Hintergrund-Tasks sauber beendet. Dies f√ºhrt zu:
- Mehrere uvicorn/python-Prozesse belegen gleichzeitig Port 8000
- MQTT-Verbindungen werden nicht ordnungsgem√§√ü geschlossen
- MockESPManager-Tasks laufen weiter ohne Parent-Prozess

#### Symptome (Beobachtet am 2025-12-26)
```
# Vor Bereinigung gefunden:
- 16 Python-Prozesse gleichzeitig laufend
- 7 Prozesse auf Port 8000 LISTENING
- 2 uvicorn.exe Prozesse
- Hunderte MQTT TIME_WAIT Verbindungen
```

#### Betroffener Code
```python
# main.py - Lifespan Manager
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup...
    yield
    # Shutdown - UNVOLLST√ÑNDIG!
    # MockESPManager Tasks werden NICHT gestoppt
    # MQTT Subscriber wird NICHT sauber beendet
```

#### Root Cause
1. `MockESPManager._heartbeat_tasks` werden bei Shutdown nicht gecancelt
2. Kein Signal-Handler f√ºr SIGTERM/SIGINT registriert
3. Keine Timeout-Protection beim Shutdown
4. WebSocket-Verbindungen werden nicht geschlossen

#### Empfohlener Fix
Siehe Plan.md Section 8 "Problem 5: Graceful Shutdown fehlt":
```python
# In main.py Shutdown-Sektion hinzuf√ºgen:
try:
    mock_esp_manager = await MockESPManager.get_instance()
    for esp_id in list(mock_esp_manager._heartbeat_tasks.keys()):
        mock_esp_manager._heartbeat_tasks[esp_id].cancel()
    logger.info("MockESPManager simulations stopped")
except Exception as e:
    logger.warning(f"MockESPManager shutdown failed: {e}")
```

#### Workaround (Aktuell)
```powershell
# Alle Python-Prozesse forciert beenden vor Neustart:
powershell -Command "Get-Process -Name python,uvicorn -ErrorAction SilentlyContinue | Stop-Process -Force"
```

---

## üü° MEDIUM: MQTT Connection Leak

### Bug F: Hunderte TIME_WAIT Verbindungen zum MQTT Broker

**Status:** ‚ö†Ô∏è OPEN (beobachtet, Root Cause = Bug E)
**Datei:** `El Servador/god_kaiser_server/src/mqtt/client.py`
**Priorit√§t:** üü° Medium - Nach Bereinigung normalisiert

#### Beschreibung
Nach unsauberem Server-Shutdown verbleiben hunderte TCP-Verbindungen im TIME_WAIT Status auf Port 1883 (MQTT).

#### Symptome (Beobachtet am 2025-12-26)
```
# Vor Bereinigung:
netstat -ano | findstr ":1883" | wc -l
‚Üí 400+ Zeilen (fast alle TIME_WAIT)

# Nach Bereinigung und Neustart:
‚Üí 4 Zeilen (2x LISTEN IPv4/IPv6 + 1 aktive Verbindung)
```

#### Root Cause
1. **Prim√§r:** Bug E (fehlendes Graceful Shutdown)
2. MQTT-Client ruft `disconnect()` nicht vor Prozess-Ende
3. TCP-Verbindungen im TIME_WAIT bleiben 2-4 Minuten bestehen (OS-default)
4. Bei h√§ufigen Server-Neustarts summieren sich die Verbindungen

#### Betroffener Code
```python
# client.py - disconnect() wird im Shutdown nicht aufgerufen
def disconnect(self):
    if self.client:
        self.client.disconnect()
        self.client.loop_stop()
```

#### Empfohlener Fix
Im Lifespan-Shutdown sicherstellen:
```python
# main.py Shutdown:
if mqtt_client:
    mqtt_client.disconnect()
    logger.info("MQTT client disconnected")
```

#### Workaround
- Server sauber beenden (nicht kill -9)
- Bei Port-Problemen: 2-4 Minuten warten bis TIME_WAIT abl√§uft
- Oder: Forciertes Beenden aller Python-Prozesse

---

## üü° MEDIUM: MQTT Verbindungs-Bug

### Bug D: Server verbindet sich nicht zum MQTT Broker nach Startup-Timeout

**Status:** ‚ö†Ô∏è OPEN  
**Datei:** `El Servador/god_kaiser_server/src/main.py:125-134` und `src/mqtt/client.py:161-216`  
**Priorit√§t:** üü° Medium - MQTT funktioniert nicht, aber Server l√§uft

#### Beschreibung
Wenn der Server gestartet wird BEVOR der MQTT-Broker verf√ºgbar ist, schl√§gt die initiale Verbindung fehl und der Server bleibt dauerhaft ohne MQTT-Verbindung - selbst nachdem der Broker gestartet wurde.

#### Symptome
- **Server zeigt:** `"mqtt_connected": false` auf `/` Endpoint
- **Server-Logs:** Kontinuierliche Warnings `MQTT broker unavailable: Connection refused - broker unavailable`
- **Health-Check:** Status `"degraded"` statt `"healthy"`
- **Mosquitto Broker:** L√§uft erfolgreich als Windows Service auf Port 1883
- **mosquitto_pub.exe:** Kann erfolgreich Nachrichten senden (CLI funktioniert)

#### Root Cause
1. Die `connect()` Methode in `client.py` wartet max 10 Sekunden auf Verbindung (Zeilen 201-212)
2. Wenn diese Timeout erreicht wird, gibt `connect()` False zur√ºck
3. In `main.py` (Zeilen 130-134): Wenn `connected=False`, werden **MQTT Handler nie registriert**
4. Obwohl `loop_start()` l√§uft und Auto-Reconnect konfiguriert ist, wird bei erfolgreicher sp√§terer Verbindung nichts abonniert

#### Betroffener Code

```python
# main.py:128-134
mqtt_client = MQTTClient.get_instance()
connected = mqtt_client.connect()

if not connected:
    logger.error("Failed to connect to MQTT broker. Server will start but MQTT is unavailable.")
else:
    # Handler werden NUR hier registriert!
    _subscriber_instance = Subscriber(...)
    _subscriber_instance.register_handler(...)
```

#### Server Log Auszug
```
INFO:     Application startup complete.
MQTT broker unavailable: Connection refused - broker unavailable. Auto-reconnect active (exponential backoff, max 60s).
MQTT broker unavailable: Connection refused - broker unavailable. Auto-reconnect active (exponential backoff, max 60s). [50 identical messages suppressed]
```

#### Empfohlener Fix
**Option 1:** Handler auch bei fehlgeschlagener Verbindung registrieren und auf `_on_connect` Callback reagieren

```python
# In main.py - Handler immer registrieren
_subscriber_instance = Subscriber(mqtt_client, max_workers=...)
# ... register_handler calls ...

# Im connect() callback resubscribe triggern
def _on_connect(self, client, userdata, flags, rc):
    if rc == 0:
        self.connected = True
        # Re-subscribe to all topics after reconnect
        if hasattr(self, '_subscriber') and self._subscriber:
            self._subscriber.subscribe_all()
```

**Option 2:** Retry-Loop beim Startup mit l√§ngerer Wartezeit

```python
# In main.py
max_retries = 3
for i in range(max_retries):
    connected = mqtt_client.connect()
    if connected:
        break
    logger.warning(f"MQTT connection attempt {i+1}/{max_retries} failed, retrying in 5s...")
    await asyncio.sleep(5)
```

#### Workaround (Aktuell)
- Sicherstellen, dass der MQTT-Broker **VOR** dem Server gestartet wird
- Oder Server neustarten nachdem Broker l√§uft

---

## üî¥ CRITICAL: Runtime Bugs (Server Crashes)

### Bug A: Token Blacklist UNIQUE Constraint Violation

**Status:** üü¢ FIXED (2025-12-26)  
**Datei:** `El Servador/god_kaiser_server/src/api/v1/auth.py:534`  
**Priorit√§t:** üî¥ Critical - Server crasht bei Token Refresh

#### Beschreibung
Wenn zwei Browser-Tabs gleichzeitig versuchen, ein abgelaufenes Token zu refreshen, tritt ein UNIQUE Constraint Fehler auf:

```
sqlite3.IntegrityError: UNIQUE constraint failed: token_blacklist.token_hash
[SQL: INSERT INTO token_blacklist (token_hash, ...) VALUES (...)]
```

#### Root Cause
1. Browser Tab 1 ruft `/api/v1/auth/refresh` auf
2. Browser Tab 2 ruft `/api/v1/auth/refresh` mit **demselben** Refresh Token auf
3. Tab 1 f√ºgt Token zur Blacklist hinzu und gibt neues Token zur√ºck
4. Tab 2 versucht **dasselbe** Token zur Blacklist hinzuzuf√ºgen ‚Üí **UNIQUE Constraint Fehler**

#### Server Log Auszug
```
INFO:     127.0.0.1:58043 - "POST /api/v1/auth/refresh HTTP/1.1" 200 OK
INFO:     127.0.0.1:58046 - "POST /api/v1/auth/refresh HTTP/1.1" 500 Internal Server Error
Failed to blacklist old refresh token: (sqlite3.IntegrityError) UNIQUE constraint failed: token_blacklist.token_hash
```

#### Empfohlener Fix
```python
# In src/api/v1/auth.py, refresh_token endpoint:
# Option 1: Try-Except um Blacklist-Insert
try:
    token_repo.add_to_blacklist(...)
except IntegrityError:
    # Token wurde bereits blacklisted - ist OK, einfach weitermachen
    db.rollback()

# Option 2: Erst pr√ºfen ob Token bereits blacklisted
if not token_repo.is_blacklisted(old_refresh_token):
    token_repo.add_to_blacklist(...)
```

---

### Bug B: ThreadPoolExecutor.shutdown() timeout Parameter

**Status:** üü¢ FIXED (2025-12-26)  
**Datei:** `El Servador/god_kaiser_server/src/mqtt/subscriber.py:272`  
**Priorit√§t:** üî¥ Critical - Server Shutdown crasht

#### Beschreibung
Beim Shutdown des Servers tritt ein TypeError auf:

```python
TypeError: ThreadPoolExecutor.shutdown() got an unexpected keyword argument 'timeout'
```

#### Root Cause
Python 3.14 hat eine andere API f√ºr `ThreadPoolExecutor.shutdown()`. Der `timeout` Parameter ist in dieser Version nicht verf√ºgbar.

#### Server Log Auszug
```
Shutdown failed: ThreadPoolExecutor.shutdown() got an unexpected keyword argument 'timeout'
  File "src/mqtt/subscriber.py", line 272, in shutdown
    self.executor.shutdown(wait=wait, timeout=timeout)
TypeError: ThreadPoolExecutor.shutdown() got an unexpected keyword argument 'timeout'
```

#### Empfohlener Fix
```python
# In src/mqtt/subscriber.py:272
# Von:
self.executor.shutdown(wait=wait, timeout=timeout)

# Nach (Python 3.9+ kompatibel):
import sys
if sys.version_info >= (3, 9):
    self.executor.shutdown(wait=wait, cancel_futures=True)
else:
    self.executor.shutdown(wait=wait)
# Oder einfach timeout entfernen:
self.executor.shutdown(wait=wait)
```

**Hinweis:** Python 3.14 ist eine Pre-Release Version. Der `timeout` Parameter wurde m√∂glicherweise in 3.9 hinzugef√ºgt und sp√§ter wieder entfernt/ge√§ndert.

---

## ~~üü° MEDIUM: Log-Spam Bugs~~ ‚úÖ FIXED

### Bug C: MQTT Log-Spam bei fehlendem Broker

**Status:** üü¢ FIXED (2025-12-26)  
**Datei:** `El Servador/god_kaiser_server/src/mqtt/client.py:321-365`  
**Priorit√§t:** üü° Medium - Log-Spam macht Server-Logs unlesbar

#### Beschreibung
Wenn kein MQTT-Broker verf√ºgbar ist, spammt der Server endlos Warning-Meldungen:

```
MQTT client disconnected unexpectedly: Unknown reason (code: 7). Auto-reconnect will attempt to restore connection...
MQTT client disconnected unexpectedly: Unknown reason (code: 7). Auto-reconnect will attempt to restore connection...
... (tausende Male)
```

#### Root Cause
Der `_on_disconnect` Callback in `client.py` loggt bei jedem Reconnect-Versuch eine Warning. Da der Auto-Reconnect kontinuierlich versucht sich zu verbinden (mit Exponential Backoff bis max 60s), werden tausende Logs generiert.

**Betroffener Code:**
```python
def _on_disconnect(self, client, userdata, rc):
    self.connected = False
    if rc == 0:
        logger.info(f"MQTT client disconnected: {reason}")
    else:
        logger.warning(  # ‚ö†Ô∏è Wird bei JEDEM Reconnect-Versuch geloggt!
            f"MQTT client disconnected unexpectedly: {reason}. "
            "Auto-reconnect will attempt to restore connection..."
        )
```

#### Server Log Auszug
```
MQTT client disconnected unexpectedly: Unknown reason (code: 7). Auto-reconnect will attempt to restore connection...
MQTT client disconnected unexpectedly: Unknown reason (code: 7). Auto-reconnect will attempt to restore connection...
... (340+ Zeilen in wenigen Minuten)
```

#### Implementierter Fix (Industrietauglich)
Rate-Limiting mit Z√§hler: Loggt ersten Disconnect als WARNING, dann alle 10 Versuche, dazwischen DEBUG.

```python
# In __init__:
self._disconnect_count = 0
self._disconnect_log_interval = 10

# In _on_disconnect:
self._disconnect_count += 1
if rc == 0:
    logger.info(f"MQTT client disconnected: {reason}")
    self._disconnect_count = 0
else:
    if self._disconnect_count == 1:
        logger.warning(f"MQTT disconnected: {reason}. Auto-reconnect enabled...")
    elif self._disconnect_count % self._disconnect_log_interval == 0:
        logger.warning(f"MQTT reconnect #{self._disconnect_count} still failing")
    else:
        logger.debug(f"MQTT reconnect #{self._disconnect_count}")
```

#### Behobene Auswirkungen
- ‚úÖ **Development:** Log-Output bleibt lesbar
- ‚úÖ **Production:** Log-Dateien wachsen nicht mehr unkontrolliert
- ‚úÖ **Debugging:** Wichtige Logs werden nicht mehr √ºberdeckt
- ‚úÖ **Troubleshooting:** DEBUG-Level erm√∂glicht vollst√§ndiges Tracing bei Bedarf

---

## 1. ~~Fehlgeschlagener Test: SHT31 Humidity Unit~~ ‚úÖ FIXED

**Status:** üü¢ FIXED (2025-12-26)  
**Datei:** `El Servador/god_kaiser_server/tests/integration/test_library_e2e_integration.py:439`  
**Priorit√§t:** üü° Medium

### Beschreibung
Der Test `TestSHT31RealProcessing::test_sht31_humidity_processing` erwartet die Einheit `%`, aber der Humidity-Processor gibt korrekt `%RH` zur√ºck.

### Fehlermeldung
```
AssertionError: assert '%RH' == '%'
  - %
  + %RH
```

### Analyse
- **Code:** `El Servador/god_kaiser_server/src/sensors/sensor_libraries/active/humidity.py:112` gibt `unit="%RH"` zur√ºck
- **Kommentar im Code (Zeile 94):** Explizit dokumentiert: `result.unit = "%RH"`
- **Fazit:** Der **Test ist falsch**, nicht der Code. `%RH` (Relative Humidity) ist die korrekte Einheit.

### Fix
```python
# tests/integration/test_library_e2e_integration.py:439
# Von:
assert result.unit == "%"
# Nach:
assert result.unit == "%RH"
```

---

## 2. Deprecation: Pydantic class Config

**Status:** ‚ö†Ô∏è WARNING  
**Dateien:**
- `El Servador/god_kaiser_server/src/api/schemas.py:15, 98, 156, 204, 277`
- `El Servador/god_kaiser_server/src/api/v1/audit.py:37`

**Priorit√§t:** üü° Medium (wird in Pydantic v3 entfernt)

### Beschreibung
```
PydanticDeprecatedSince20: Support for class-based `config` is deprecated, 
use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0.
```

### Fix
```python
# Von:
class MyModel(BaseModel):
    class Config:
        from_attributes = True

# Nach:
from pydantic import ConfigDict

class MyModel(BaseModel):
    model_config = ConfigDict(from_attributes=True)
```

---

## 3. Deprecation: datetime.utcnow()

**Status:** ‚ö†Ô∏è WARNING  
**Dateien:**
- `src/db/repositories/actuator_repo.py:212`
- `src/db/repositories/sensor_repo.py:214`
- `src/db/repositories/system_config_repo.py:200`
- `tests/unit/test_repositories_actuator.py:115`
- `tests/unit/test_repositories_sensor.py:230, 260`

**Priorit√§t:** üü° Medium (wird in Python 3.12+ deprecated)

### Beschreibung
```
DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal.
Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
```

### Fix
```python
# Von:
from datetime import datetime
timestamp = datetime.utcnow()

# Nach:
from datetime import datetime, UTC
timestamp = datetime.now(UTC)
```

---

## 4. Deprecation: asyncio.iscoroutinefunction

**Status:** ‚ö†Ô∏è WARNING  
**Betroffene Libraries:** pytest-asyncio, FastAPI/Starlette  
**Priorit√§t:** üîµ Low (externe Libraries)

### Beschreibung
```
DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated and slated for removal 
in Python 3.16; use inspect.iscoroutinefunction() instead
```

### L√∂sung
- Warten auf Updates von `pytest-asyncio` und `fastapi`
- Keine direkte Code-√Ñnderung erforderlich
- Ca. 181.000+ Warnungen (von Libraries generiert)

---

## 5. Coverage Collection fehlgeschlagen

**Status:** ‚ö†Ô∏è WARNING  
**Priorit√§t:** üîµ Low

### Beschreibung
```
CoverageWarning: Module god_kaiser_server was never imported. (module-not-imported)
CoverageWarning: No data was collected. (no-data-collected)
```

### Ursache
Die `pyproject.toml` definiert `packages = [{include = "god_kaiser_server", from = "src"}]`, aber die Verzeichnisstruktur ist `src/` (nicht `src/god_kaiser_server/`).

### Fix-Optionen
1. `pyproject.toml` anpassen:
```toml
[tool.coverage.run]
source = ["src"]
```

2. Oder Coverage-Source anpassen f√ºr Tests:
```bash
poetry run pytest tests/ --cov=src --cov-report=term-missing
```

---

## 6. Sicherheitshinweise (Development Only)

**Status:** ‚ÑπÔ∏è INFO  
**Priorit√§t:** üîµ Low (nur Development)

### A) Default JWT Secret Key
```
SECURITY: Using default JWT secret key (OK for development only). 
Change JWT_SECRET_KEY in production!
```

**Aktion f√ºr Production:** `.env` mit `JWT_SECRET_KEY=<secure-random-key>` erstellen

### B) MQTT TLS deaktiviert
```
MQTT TLS is disabled. MQTT authentication credentials will be sent in plain text. 
Enable MQTT_USE_TLS for secure credential distribution.
```

**Aktion f√ºr Production:** `MQTT_USE_TLS=true` in `.env` setzen

---

## 7. √úbersprungene Tests (6 Tests)

**Status:** ‚ÑπÔ∏è INFO (erwartet)

| Test | Grund |
|------|-------|
| `test_communication.py` (2x) | "Real ESP32 MQTT client not yet implemented" |
| `test_communication.py` (2x) | "ESP32_TEST_DEVICE_ID not set - skipping real hardware tests" |
| `test_mqtt_auth_service.py` (2x) | "Unix permissions not supported on Windows" |

**Keine Aktion erforderlich** - Diese Tests erfordern spezielle Umgebungen.

---

## Test-Ergebnisse √úbersicht

```
============================= test session starts =============================
platform win32 -- Python 3.14.0, pytest-8.4.2
collected 781 items
========= 775 passed, 6 skipped, 183488 warnings in 153.32s (0:02:33) =========
```

### Server Status (Stand: 2025-12-26 12:18 UTC)
- **URL:** http://localhost:8000
- **Health-Check:** ‚úÖ `{"status":"healthy","version":"2.0.0","uptime_seconds":68}`
- **Environment:** development
- **MQTT Broker:** ‚úÖ L√§uft (Windows Service `mosquitto` auf Port 1883, PID 4776)
- **MQTT Server-Verbindung:** ‚úÖ `"mqtt_connected": true`
- **MQTT Connections:** 4 (normal: 2x LISTEN + 1 aktive Verbindung)
- **TIME_WAIT Connections:** 0 (nach Bereinigung normalisiert)
- **MQTT Rate-Limiter:** ‚úÖ Funktioniert (1 Log/min statt 100+/min)
- **Hinweis:** Server wurde nach Bereinigung aller Zombie-Prozesse neu gestartet

---

## N√§chste Schritte

1. ~~**[CRITICAL]** Fix Token Blacklist UNIQUE Constraint ‚Üí Try-Except oder Check before Insert~~ ‚úÖ DONE
2. ~~**[CRITICAL]** Fix ThreadPoolExecutor.shutdown() ‚Üí `timeout` Parameter entfernen~~ ‚úÖ DONE
3. ~~**[HIGH]** Fix Test `test_sht31_humidity_processing` ‚Üí `%RH` statt `%`~~ ‚úÖ DONE
4. ~~**[MEDIUM]** Fix MQTT Log-Spam ‚Üí Rate-Limiting implementieren~~ ‚úÖ DONE
5. **[CRITICAL]** Fix Graceful Shutdown ‚Üí MockESPManager Tasks canceln, MQTT disconnect (Bug E) ‚ö° NEU
6. **[MEDIUM]** Fix MQTT Connection Leak ‚Üí disconnect() im Shutdown aufrufen (Bug F) ‚ö° NEU
7. **[MEDIUM]** Fix MQTT Reconnect-Bug ‚Üí Handler bei erfolgreicher Reconnection re-subscriben (Bug D)
8. **[MEDIUM]** Pydantic `class Config` zu `ConfigDict` migrieren
9. **[MEDIUM]** `datetime.utcnow()` zu `datetime.now(UTC)` migrieren
10. **[LOW]** Coverage-Konfiguration korrigieren

---

## Historie

| Datum | Aktion |
|-------|--------|
| 2025-12-26 | Initiale Analyse: 781 Tests, 1 Fehler, 6 √ºbersprungen |
| 2025-12-26 | Test `test_sht31_humidity_processing` gefixt: `%` ‚Üí `%RH` |
| 2025-12-26 | 2 CRITICAL Runtime Bugs entdeckt bei Frontend-Browser-Test |
| 2025-12-26 | Token Blacklist Bug gefixt: Cache User-Data vor DB-Operation, Rollback bei Fehler |
| 2025-12-26 | ThreadPoolExecutor Bug gefixt: timeout-Parameter entfernt, cancel_futures stattdessen |
| 2025-12-26 | **Alle 781 Tests bestehen jetzt (0 Fehler, 6 √ºbersprungen)** |
| 2025-12-26 | **3 kritische Bugs gefixt in dieser Session** |
| 2025-12-26 | Bug C entdeckt: MQTT Log-Spam bei fehlendem Broker (tausende Warnings) |
| 2025-12-26 | Bug C gefixt: Rate-Limiting f√ºr MQTT Disconnect-Logs implementiert |
| 2025-12-26 | Tests erneut verifiziert: 775 passed, 6 skipped (153s) |
| 2025-12-26 | **4 Bugs in dieser Session gefixt (3 critical + 1 medium)** |
| 2025-12-26 | System-Verifizierung: Server startet, alle 775 Tests bestanden, MQTT Rate-Limiter funktioniert (47/min ‚Üí 1/min) |
| 2025-12-26 | Bug D entdeckt: MQTT verbindet sich nicht nach Startup-Timeout obwohl Broker l√§uft |
| 2025-12-26 | System-Status: Server HTTP/WebSocket ‚úÖ, MQTT ‚ùå (Server vor Broker gestartet), Frontend ‚úÖ |
| 2025-12-26 | Bug E entdeckt: Zombie-Prozesse und fehlender Graceful Shutdown |
| 2025-12-26 | Bug F entdeckt: MQTT Connection Leak (TIME_WAIT Connections) |
| 2025-12-26 | Server neugestartet nach Bereinigung aller Prozesse - MQTT verbunden ‚úÖ |
